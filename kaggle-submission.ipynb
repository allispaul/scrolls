{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{"_uuid":"27cf84f6-77f3-4b7e-ac41-ea98bef228f8","_cell_guid":"dccdbe9f-23bd-4b84-818d-e11f13fdba12","trusted":true}},{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport json\nfrom collections import defaultdict\nimport multiprocessing as mp\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import Dict, List, Optional, Tuple\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport pandas as pd\nimport PIL.Image as Image\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.exceptions import UndefinedMetricWarning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom tqdm import tqdm","metadata":{"_uuid":"7af59016-7a75-4e16-bde6-c0810ad38e1a","_cell_guid":"8507994d-52ca-4a19-beeb-3ab1a7937561","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:23:38.942552Z","iopub.execute_input":"2023-06-11T04:23:38.943437Z","iopub.status.idle":"2023-06-11T04:23:43.122496Z","shell.execute_reply.started":"2023-06-11T04:23:38.943365Z","shell.execute_reply":"2023-06-11T04:23:43.121036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up data","metadata":{"_uuid":"ba76834a-f526-4849-8543-6d79070ef2c4","_cell_guid":"ab89042f-1641-49e2-98e3-6e24b10c4d76","trusted":true}},{"cell_type":"code","source":"BASE_PREFIX = Path('/kaggle/input/vesuvius-challenge/')\nPREFIX = BASE_PREFIX / 'train/1/'\nBUFFER = 45  # Buffer size in x and y direction\nZ_START = 8 # First slice in the z direction to use\nZ_DIM = 24  # Number of slices in the z direction\nTRAINING_EPOCHS = 30000\nVALIDATION_EPOCHS = 500\nLEARNING_RATE = 0.02\nBATCH_SIZE = 32\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","metadata":{"_uuid":"73b4981e-7573-4373-b9ef-6e9e8f82e2fe","_cell_guid":"975303e1-77d4-4954-9872-0b5f176bfbad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:37:13.297557Z","iopub.execute_input":"2023-06-11T04:37:13.298346Z","iopub.status.idle":"2023-06-11T04:37:13.308038Z","shell.execute_reply.started":"2023-06-11T04:37:13.298297Z","shell.execute_reply":"2023-06-11T04:37:13.306730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Optional\nclass SubvolumeDataset(data.Dataset):\n    \"\"\"Dataset containing cubical subvolumes of image stack, with the possibility\n    of data augmentation through random flips and rotations.\n    \"\"\"\n    def __init__(self,\n                 image_stack: torch.Tensor,\n                 label: Optional[torch.Tensor],\n                 pixels: torch.Tensor,\n                 buffer: int,\n                 xy_flip_prob: float = 0.0,\n                 z_flip_prob: float = 0.0,\n                 xy_rot_prob: float = 0.0):\n        \"\"\"Create a new SubvolumeDataset.\n        \n        Args:\n          image_stack: 3D image data, as a tensor of voxel values of shape\n            (z_dim, y_dim, x_dim).\n          label: ink labels for the image stack. A tensor of shape (y_dim, x_dim).\n            For testing data, instead pass label=None.\n          pixels: Tensor listing pixels to use as centers of subvolumes, of\n            shape (num_pixels, 2). Each row of pixels gives coordinates (y, x) of\n            a single subvolume center.\n          buffer: radius of each subvolume in the x and y direction. Thus, each\n            subvolume has shape (z_dim, 2*buffer+1, 2*buffer+1).\n          xy_flip_prob: Probability of reflecting each item in the x and y\n            directions (independently).\n          z_flip_prob: Probability of reflecting each item in the z direction.\n          xy_rot_prob: Probability of rotating item by 90 degrees in the xy\n            plane. If this check is met, there's a 50% chance of a clockwise\n            rotation and a 50% chance of a counterclockwise rotation.\n        \"\"\"\n        super().__init__()\n        self.image_stack = image_stack\n        self.label = label\n        self.pixels = pixels\n        self.buffer = buffer\n        self.xy_flip_prob = xy_flip_prob\n        self.z_flip_prob = z_flip_prob\n        self.xy_rot_prob = xy_rot_prob\n    \n    def __len__(self):\n        return len(self.pixels)\n    \n    def __getitem__(self, index):\n        \"\"\"Get a subvolume from the dataset.\n        \n        If the dataset was defined without label data, the returned label will\n        be -1.\n        \"\"\"\n        # Note! torch.flip returns a copy, not a view -- thus, we expect this\n        # to be slower than the vanilla SubvolumeDataset.\n        # Flipping in numpy and then converting to torch.Tensor doesn't work,\n        # since torch.Tensor can't take a numpy array with a negative stride.\n        y, x = self.pixels[index]\n        subvolume = self.image_stack[\n            :, y-self.buffer:y+self.buffer+1, x-self.buffer:x+self.buffer+1\n        ].reshape(1, -1, self.buffer*2+1, self.buffer*2+1) # -> [1, z, y, x]\n        \n        # Perform transforms\n        if random.random() < self.xy_flip_prob:\n            subvolume = torch.flip(subvolume, (2,))\n        if random.random() < self.xy_flip_prob:\n            subvolume = torch.flip(subvolume, (3,))\n        if random.random() < self.z_flip_prob:\n            subvolume = torch.flip(subvolume, (1,))\n        if random.random() < self.xy_rot_prob:\n            if random.random() < 0.5:\n                subvolume = torch.rot90(subvolume, k=1, dims=(2, 3))\n            else:\n                subvolume = torch.rot90(subvolume, k=3, dims=(2, 3))\n        \n        if self.label is None:\n            inklabel = -1\n        else:\n            inklabel = self.label[y, x].view(1)\n            \n        return subvolume, inklabel\n    \n    def set_probs(self, xy_flip_prob, z_flip_prob, xy_rot_prob):\n        \"\"\"Set probabilities of data augmentation transforms.\"\"\"\n        self.xy_flip_prob = xy_flip_prob\n        self.z_flip_prob = z_flip_prob\n        self.xy_rot_prob = xy_rot_prob","metadata":{"_uuid":"955d8793-68a8-462f-8a8d-c0b470c77188","_cell_guid":"47e3887d-dbb9-4446-b8d1-39f665c2733a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:30:43.392953Z","iopub.execute_input":"2023-06-11T04:30:43.393342Z","iopub.status.idle":"2023-06-11T04:30:43.411645Z","shell.execute_reply.started":"2023-06-11T04:30:43.393306Z","shell.execute_reply":"2023-06-11T04:30:43.410550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up model","metadata":{"_uuid":"8a989895-3ec4-471d-9627-1c655494dce8","_cell_guid":"20971b1b-bb27-454d-9b05-88f07d692816","trusted":true}},{"cell_type":"code","source":"class InkDetector(torch.nn.Module):\n    def __init__(self, out_channels=1):\n        super().__init__()\n\n        filters = [16, 32, 64]\n        paddings = [1, 1, 1]\n        kernel_sizes = [3, 3, 3]\n        strides = [2, 2, 2]\n        \n        layers = []\n        in_channels = 1\n        for num_filters, padding, kernel_size, stride in zip(filters, paddings, kernel_sizes, strides):\n            layers.extend([\n                nn.Conv3d(\n                    in_channels=in_channels,\n                    out_channels=num_filters,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                ),\n                nn.ReLU(inplace=True),\n                torch.nn.BatchNorm3d(num_features=num_filters)\n            ])\n            in_channels = num_filters\n        layers.append(nn.AdaptiveAvgPool3d(1))\n        layers.append(nn.Flatten())\n\n        self.encoder = nn.Sequential(*layers)\n        self.decoder = nn.Sequential(\n            nn.Linear(in_channels, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, out_channels)\n        )\n\n    def forward(self, x):\n        features = self.encoder(x)\n        return self.decoder(features)","metadata":{"_uuid":"f0777d17-4548-4170-bd62-1c2dd568d98f","_cell_guid":"03a1a216-2a1f-48ce-8a7d-20954b679c2e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:25:28.591725Z","iopub.execute_input":"2023-06-11T04:25:28.592495Z","iopub.status.idle":"2023-06-11T04:25:28.606328Z","shell.execute_reply.started":"2023-06-11T04:25:28.592453Z","shell.execute_reply":"2023-06-11T04:25:28.605137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = InkDetector().to(DEVICE)\nmodel.load_state_dict(torch.load('/kaggle/input/pretrained-model-for-ink-detection/InkDetector_full_dataset_augmented_pretrained_100000_epochs.pt'))","metadata":{"_uuid":"cbdceb9d-be82-425d-b158-118f5e6ab144","_cell_guid":"bed9f3e2-4d17-4871-9f49-cfacaa51a175","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:26:20.983808Z","iopub.execute_input":"2023-06-11T04:26:20.984914Z","iopub.status.idle":"2023-06-11T04:26:21.036757Z","shell.execute_reply.started":"2023-06-11T04:26:20.984871Z","shell.execute_reply":"2023-06-11T04:26:21.035650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate","metadata":{"_uuid":"7440c803-791a-427e-ab23-4bfe42e6c9a6","_cell_guid":"31e44238-1517-454d-a8e1-9610f6c73f95","trusted":true}},{"cell_type":"code","source":"test_path = BASE_PREFIX / \"test\"\ntest_fragments = [test_path / fragment_name for fragment_name in test_path.iterdir()]\nprint(\"All fragments:\", test_fragments)","metadata":{"_uuid":"aecd6fa8-e2bc-4d47-a142-82b77ec2e561","_cell_guid":"63bde930-a0e3-431b-b1ce-dcdb4585672d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:26:36.221577Z","iopub.execute_input":"2023-06-11T04:26:36.221981Z","iopub.status.idle":"2023-06-11T04:26:36.229984Z","shell.execute_reply.started":"2023-06-11T04:26:36.221945Z","shell.execute_reply":"2023-06-11T04:26:36.228824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom typing import Union, Optional, Tuple, List\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nImage.MAX_IMAGE_PIXELS = None\n\ndef get_rect_dset(\n    fragment_path: Union[Path, str],\n    z_start: int,\n    z_dim: int,\n    buffer: int,\n    rect: Optional[Tuple[int]] = None,\n    shuffle: bool = False,\n) -> data.Dataset:\n    \"\"\"Get a dataset consisting of a rectangle from a single fragment.\n    \n    Args:\n      fragment_path: Path to folder containing fragment data (e.g., 'data/train/1')\n      z_start: Lowest z-value to use from the fragment. Should be between 0\n        and 64. \n      z_dim: Number of z-values to use.\n      buffer: Radius of subvolumes in x and y directions. Thus, each item in\n        the dataset will be a subvolume of size 2*buffer+1 x 2*buffer+1 x z_dim.\n      rect: Rectangle to use from the fragment. Should be a tuple of the form:\n          (top_left_corner_x, top_left_corner_y, width, height)\n        Use show_labels_with_rects() to double-check the rectangle. If rect is\n        None, the whole dataset will be used.\n      shuffle: Whether to shuffle the dataset before returning it.\n    \n    Returns:\n      A dataset consisting of subvolumes from the given fragment inside the\n      given rectangle.\n    \"\"\"\n    # clean input\n    fragment_path = Path(fragment_path)\n    \n    images = [\n        np.array(Image.open(filename), dtype=np.float32)/65535.0\n        for filename\n        in tqdm(\n            sorted((fragment_path / \"surface_volume\").glob(\"*.tif\"))[z_start : z_start + z_dim],\n            desc=f\"Loading fragment from {fragment_path}\"\n        )\n    ]\n        \n    # turn images to tensors\n    image_stack = torch.stack([torch.from_numpy(image) for image in images], \n                              dim=0)\n\n    # get mask and labels\n    mask = np.array(Image.open(fragment_path / \"mask.png\").convert('1'))\n    if os.path.exists(fragment_path / \"inklabels.png\"):\n        label = torch.from_numpy(\n            np.array(Image.open(fragment_path / \"inklabels.png\"))\n        ).float()\n    else:\n        label = None\n\n    # Create a Boolean array of the same shape as the bitmask, initially all True\n    not_border = np.zeros(mask.shape, dtype=bool)\n    not_border[buffer:mask.shape[0]-buffer, buffer:mask.shape[1]-buffer] = True\n    arr_mask = mask * not_border\n    if rect is not None:\n        inside_rect = np.zeros(mask.shape, dtype=bool) * arr_mask\n        # Sets all indexes with inside_rect array to True\n        inside_rect[rect[1]:rect[1]+rect[3]+1, rect[0]:rect[0]+rect[2]+1] = True\n        # Set the pixels within the inside_rect to False\n        pixels = torch.tensor(np.argwhere(inside_rect))\n    else:\n        pixels = torch.tensor(np.argwhere(arr_mask))\n        \n    if shuffle:\n        perm = torch.randperm(len(pixels))\n        pixels = pixels[perm]\n\n    # define dataset\n    return SubvolumeDataset(image_stack, label, pixels, buffer)\n\ndef predict_on_test_fragments(model: torch.nn.Module,\n                              fragment_path: Union[Path, str],\n                              z_start: int,\n                              z_dim: int,\n                              buffer: int, \n                              batch_size: int,\n                              decision_boundary: float = 0.4) -> np.ndarray:\n    \"\"\"Generate predictions on a fragment.\n    \n    Args:\n      model: Model to use for prediction.\n      fragment_path: Path to fragment (e.g. '/data/test/a').\n      z_start: z-value of lowest fragment layer to use.\n      z_dim: Number of layers to use.\n      buffer: Radius of subvolumes in x and y direction. Thus, each subvolume\n        is an array of shape (z_dim, 2*buffer+1, 2*buffer+1).\n      batch_size: Number of subvolumes in one batch.\n      decision_boundary: Decision boundary for predictions. If the model\n        returns a probability above this number for a given pixel, we classify\n        that pixel as containing ink.\n    \n    Returns:\n      Predictions on the fragment, as a boolean NumPy array.\n    \"\"\"\n    fragment_path = Path(fragment_path)\n    model.eval()\n    outputs = []\n    test_dset = get_rect_dset(fragment_path, z_start=z_start,\n                              z_dim=z_dim, buffer=buffer)\n    test_loader = data.DataLoader(test_dset, batch_size=batch_size, shuffle=False)\n    with torch.no_grad():\n        for i, (subvolumes, _) in enumerate(tqdm(test_loader, desc=f\"Predicting on fragment {fragment_path}\")):\n            output = model(subvolumes.to(DEVICE)).view(-1).sigmoid().cpu().numpy()\n            outputs.append(output)\n    image_shape = test_dset.image_stack[0].shape\n\n    pred_image = np.zeros(image_shape, dtype=np.uint8)\n    outputs = np.concatenate(outputs)\n    for (y, x, _), prob in zip(test_dset.pixels[:outputs.shape[0]], outputs):\n        pred_image[y, x] = prob > decision_boundary\n    pred_images.append(pred_image)\n\n    print(\"Finished with fragment\", test_fragment)\n    return pred_image\n                                            \n    \ndef rle(output: np.ndarray) -> str:\n    \"\"\"Turn a NumPy array of booleans to a run-length encoded string.\"\"\"\n    flat_img = np.where(output > 0.4, 1, 0).astype(np.uint8)\n    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n    starts_ix = np.where(starts)[0] + 2\n    ends_ix = np.where(ends)[0] + 2\n    lengths = ends_ix - starts_ix\n    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n\n                                                \ndef make_csv(pred_images: List[np.ndarray], save_path: Union[Path, str, None]):\n    \"\"\"Save a list of predicted images as a run-length encoded CSV file.\n    \n    Args:\n      pred_images: List of two NumPy arrays of predictions (which we assume\n        correspond to test fragments a and b respectively).\n      save_path: Path to save CSV file (e.g. '/kaggle/working/submission.csv').\n        If None, the file will not be saved.\n    Returns:\n      A DataFrame containing run-length encodings of the two fragments.\n    \"\"\"\n    submission = defaultdict(list)\n    for fragment_id, fragment_name in enumerate(['a', 'b']):\n        submission[\"Id\"].append(fragment_name)\n        submission[\"Predicted\"].append(rle(pred_images[fragment_id]))\n    dataframe = pd.DataFrame.from_dict(submission)\n    dataframe.to_csv(save_path, index=False)\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2023-06-11T04:37:17.918511Z","iopub.execute_input":"2023-06-11T04:37:17.918965Z","iopub.status.idle":"2023-06-11T04:37:17.953945Z","shell.execute_reply.started":"2023-06-11T04:37:17.918927Z","shell.execute_reply":"2023-06-11T04:37:17.952685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_images = []\nmodel.eval()\nfor test_fragment in test_fragments:\n    pred_images.append(predict_on_test_fragments(model, test_fragment, Z_START, Z_DIM, BUFFER, BATCH_SIZE,\n                                                 decision_boundary=0.4))\nmake_csv(pred_images, '/kaggle/input/vesuvius-challenge/submission.csv')","metadata":{"_uuid":"cf6421bc-8930-4a97-b03d-9ad35a34276d","_cell_guid":"98b187d3-1728-47e3-b2ef-333f04d9a907","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-11T04:37:18.358419Z","iopub.execute_input":"2023-06-11T04:37:18.359724Z","iopub.status.idle":"2023-06-11T04:39:46.974885Z","shell.execute_reply.started":"2023-06-11T04:37:18.359669Z","shell.execute_reply":"2023-06-11T04:39:46.973048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(pred_images[0], cmap='gray')","metadata":{"_uuid":"84cb4118-e2e2-4efb-91a9-af0e270e0a7e","_cell_guid":"1da918ef-1f34-4d2a-b7f3-9f826b9de1b6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{"_uuid":"8bf1d812-b298-4261-8e25-b44f07dd9002","_cell_guid":"ed83ebc7-a0b1-4434-aba0-875bbaea0b52","trusted":true}},{"cell_type":"code","source":"def rle(output):\n    flat_img = np.where(output > 0.4, 1, 0).astype(np.uint8)\n    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n    starts_ix = np.where(starts)[0] + 2\n    ends_ix = np.where(ends)[0] + 2\n    lengths = ends_ix - starts_ix\n    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))","metadata":{"_uuid":"54905e04-f6d1-40bc-9645-32ff0ee09391","_cell_guid":"17314185-11ba-448e-915c-97df95956cd8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = defaultdict(list)\nfor fragment_id, fragment_name in enumerate(test_fragments):\n    submission[\"Id\"].append(fragment_name.name)\n    submission[\"Predicted\"].append(rle(pred_images[fragment_id]))\n\npd.DataFrame.from_dict(submission).to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"_uuid":"37c064ea-e643-49a5-b355-01ba56855184","_cell_guid":"d833e488-1a89-4a7d-a733-a193f54a3130","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame.from_dict(submission)","metadata":{"_uuid":"b86d9f0f-bfe1-4192-8bce-c8c3decd70dc","_cell_guid":"a93039e8-0d4d-46d0-83ad-0760763f00e3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}