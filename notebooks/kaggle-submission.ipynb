{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0eb4ea",
   "metadata": {
    "_cell_guid": "dccdbe9f-23bd-4b84-818d-e11f13fdba12",
    "_uuid": "27cf84f6-77f3-4b7e-ac41-ea98bef228f8",
    "papermill": {
     "duration": 0.006167,
     "end_time": "2023-06-14T18:54:30.138751",
     "exception": false,
     "start_time": "2023-06-14T18:54:30.132584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517b759d",
   "metadata": {
    "_cell_guid": "8507994d-52ca-4a19-beeb-3ab1a7937561",
    "_uuid": "7af59016-7a75-4e16-bde6-c0810ad38e1a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:30.151115Z",
     "iopub.status.busy": "2023-06-14T18:54:30.150671Z",
     "iopub.status.idle": "2023-06-14T18:54:33.483990Z",
     "shell.execute_reply": "2023-06-14T18:54:33.482872Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.342733,
     "end_time": "2023-06-14T18:54:33.486786",
     "exception": false,
     "start_time": "2023-06-14T18:54:30.144053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270d5ef",
   "metadata": {
    "_cell_guid": "ab89042f-1641-49e2-98e3-6e24b10c4d76",
    "_uuid": "ba76834a-f526-4849-8543-6d79070ef2c4",
    "papermill": {
     "duration": 0.004955,
     "end_time": "2023-06-14T18:54:33.497103",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.492148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1663fd",
   "metadata": {
    "_cell_guid": "975303e1-77d4-4954-9872-0b5f176bfbad",
    "_uuid": "73b4981e-7573-4373-b9ef-6e9e8f82e2fe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.508808Z",
     "iopub.status.busy": "2023-06-14T18:54:33.508318Z",
     "iopub.status.idle": "2023-06-14T18:54:33.572081Z",
     "shell.execute_reply": "2023-06-14T18:54:33.570420Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072078,
     "end_time": "2023-06-14T18:54:33.574238",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.502160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BASE_PREFIX = Path('/kaggle/input/vesuvius-challenge/')\n",
    "PREFIX = BASE_PREFIX / 'train/1/'\n",
    "BUFFER = 45  # Buffer size in x and y direction\n",
    "Z_START = 8 # First slice in the z direction to use\n",
    "Z_DIM = 24  # Number of slices in the z direction\n",
    "TRAINING_EPOCHS = 30000\n",
    "VALIDATION_EPOCHS = 500\n",
    "LEARNING_RATE = 0.02\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557354bb",
   "metadata": {
    "_cell_guid": "47e3887d-dbb9-4446-b8d1-39f665c2733a",
    "_uuid": "955d8793-68a8-462f-8a8d-c0b470c77188",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.586459Z",
     "iopub.status.busy": "2023-06-14T18:54:33.586115Z",
     "iopub.status.idle": "2023-06-14T18:54:33.605402Z",
     "shell.execute_reply": "2023-06-14T18:54:33.604307Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028106,
     "end_time": "2023-06-14T18:54:33.607676",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.579570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class SubvolumeDataset(data.Dataset):\n",
    "    \"\"\"Dataset containing cubical subvolumes of image stack, with the possibility\n",
    "    of data augmentation through random flips and rotations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_stack: torch.Tensor,\n",
    "                 label: Optional[torch.Tensor],\n",
    "                 pixels: torch.Tensor,\n",
    "                 buffer: int,\n",
    "                 xy_flip_prob: float = 0.0,\n",
    "                 z_flip_prob: float = 0.0,\n",
    "                 xy_rot_prob: float = 0.0):\n",
    "        \"\"\"Create a new SubvolumeDataset.\n",
    "        \n",
    "        Args:\n",
    "          image_stack: 3D image data, as a tensor of voxel values of shape\n",
    "            (z_dim, y_dim, x_dim).\n",
    "          label: ink labels for the image stack. A tensor of shape (y_dim, x_dim).\n",
    "            For testing data, instead pass label=None.\n",
    "          pixels: Tensor listing pixels to use as centers of subvolumes, of\n",
    "            shape (num_pixels, 2). Each row of pixels gives coordinates (y, x) of\n",
    "            a single subvolume center.\n",
    "          buffer: radius of each subvolume in the x and y direction. Thus, each\n",
    "            subvolume has shape (z_dim, 2*buffer+1, 2*buffer+1).\n",
    "          xy_flip_prob: Probability of reflecting each item in the x and y\n",
    "            directions (independently).\n",
    "          z_flip_prob: Probability of reflecting each item in the z direction.\n",
    "          xy_rot_prob: Probability of rotating item by 90 degrees in the xy\n",
    "            plane. If this check is met, there's a 50% chance of a clockwise\n",
    "            rotation and a 50% chance of a counterclockwise rotation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_stack = image_stack\n",
    "        self.label = label\n",
    "        self.pixels = pixels\n",
    "        self.buffer = buffer\n",
    "        self.xy_flip_prob = xy_flip_prob\n",
    "        self.z_flip_prob = z_flip_prob\n",
    "        self.xy_rot_prob = xy_rot_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pixels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get a subvolume from the dataset.\n",
    "        \n",
    "        If the dataset was defined without label data, the returned label will\n",
    "        be -1.\n",
    "        \"\"\"\n",
    "        # Note! torch.flip returns a copy, not a view -- thus, we expect this\n",
    "        # to be slower than the vanilla SubvolumeDataset.\n",
    "        # Flipping in numpy and then converting to torch.Tensor doesn't work,\n",
    "        # since torch.Tensor can't take a numpy array with a negative stride.\n",
    "        y, x = self.pixels[index]\n",
    "        subvolume = self.image_stack[\n",
    "            :, y-self.buffer:y+self.buffer+1, x-self.buffer:x+self.buffer+1\n",
    "        ].reshape(1, -1, self.buffer*2+1, self.buffer*2+1) # -> [1, z, y, x]\n",
    "        \n",
    "        # Perform transforms\n",
    "        if random.random() < self.xy_flip_prob:\n",
    "            subvolume = torch.flip(subvolume, (2,))\n",
    "        if random.random() < self.xy_flip_prob:\n",
    "            subvolume = torch.flip(subvolume, (3,))\n",
    "        if random.random() < self.z_flip_prob:\n",
    "            subvolume = torch.flip(subvolume, (1,))\n",
    "        if random.random() < self.xy_rot_prob:\n",
    "            if random.random() < 0.5:\n",
    "                subvolume = torch.rot90(subvolume, k=1, dims=(2, 3))\n",
    "            else:\n",
    "                subvolume = torch.rot90(subvolume, k=3, dims=(2, 3))\n",
    "        \n",
    "        if self.label is None:\n",
    "            inklabel = -1\n",
    "        else:\n",
    "            inklabel = self.label[y, x].view(1)\n",
    "            \n",
    "        return subvolume, inklabel\n",
    "    \n",
    "    def set_probs(self, xy_flip_prob, z_flip_prob, xy_rot_prob):\n",
    "        \"\"\"Set probabilities of data augmentation transforms.\"\"\"\n",
    "        self.xy_flip_prob = xy_flip_prob\n",
    "        self.z_flip_prob = z_flip_prob\n",
    "        self.xy_rot_prob = xy_rot_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc836e68",
   "metadata": {
    "_cell_guid": "20971b1b-bb27-454d-9b05-88f07d692816",
    "_uuid": "8a989895-3ec4-471d-9627-1c655494dce8",
    "papermill": {
     "duration": 0.005073,
     "end_time": "2023-06-14T18:54:33.618015",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.612942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5072d85c",
   "metadata": {
    "_cell_guid": "03a1a216-2a1f-48ce-8a7d-20954b679c2e",
    "_uuid": "f0777d17-4548-4170-bd62-1c2dd568d98f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.631310Z",
     "iopub.status.busy": "2023-06-14T18:54:33.629722Z",
     "iopub.status.idle": "2023-06-14T18:54:33.641807Z",
     "shell.execute_reply": "2023-06-14T18:54:33.640796Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020918,
     "end_time": "2023-06-14T18:54:33.644209",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.623291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InkDetector(torch.nn.Module):\n",
    "    def __init__(self, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        filters = [16, 32, 64]\n",
    "        paddings = [1, 1, 1]\n",
    "        kernel_sizes = [3, 3, 3]\n",
    "        strides = [2, 2, 2]\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for num_filters, padding, kernel_size, stride in zip(filters, paddings, kernel_sizes, strides):\n",
    "            layers.extend([\n",
    "                nn.Conv3d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=num_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "                torch.nn.BatchNorm3d(num_features=num_filters)\n",
    "            ])\n",
    "            in_channels = num_filters\n",
    "        layers.append(nn.AdaptiveAvgPool3d(1))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_channels, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b456075f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.657275Z",
     "iopub.status.busy": "2023-06-14T18:54:33.655695Z",
     "iopub.status.idle": "2023-06-14T18:54:33.661160Z",
     "shell.execute_reply": "2023-06-14T18:54:33.660170Z"
    },
    "papermill": {
     "duration": 0.014064,
     "end_time": "2023-06-14T18:54:33.663549",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.649485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49beb199",
   "metadata": {
    "_cell_guid": "bed9f3e2-4d17-4871-9f49-cfacaa51a175",
    "_uuid": "cbdceb9d-be82-425d-b158-118f5e6ab144",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.675508Z",
     "iopub.status.busy": "2023-06-14T18:54:33.674866Z",
     "iopub.status.idle": "2023-06-14T18:54:33.680325Z",
     "shell.execute_reply": "2023-06-14T18:54:33.679384Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014032,
     "end_time": "2023-06-14T18:54:33.682670",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.668638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_RUN:\n",
    "    model = InkDetector().to(DEVICE)\n",
    "    model.load_state_dict(torch.load('/kaggle/input/pretrained-model-for-ink-detection/InkDetector_full_dataset_augmented_pretrained_100000_epochs.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a650be",
   "metadata": {
    "_cell_guid": "31e44238-1517-454d-a8e1-9610f6c73f95",
    "_uuid": "7440c803-791a-427e-ab23-4bfe42e6c9a6",
    "papermill": {
     "duration": 0.005046,
     "end_time": "2023-06-14T18:54:33.692990",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.687944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b099db",
   "metadata": {
    "_cell_guid": "63bde930-a0e3-431b-b1ce-dcdb4585672d",
    "_uuid": "aecd6fa8-e2bc-4d47-a142-82b77ec2e561",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.704560Z",
     "iopub.status.busy": "2023-06-14T18:54:33.704274Z",
     "iopub.status.idle": "2023-06-14T18:54:33.713624Z",
     "shell.execute_reply": "2023-06-14T18:54:33.712638Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.017839,
     "end_time": "2023-06-14T18:54:33.715957",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.698118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All fragments: [PosixPath('/kaggle/input/vesuvius-challenge/test/b'), PosixPath('/kaggle/input/vesuvius-challenge/test/a')]\n"
     ]
    }
   ],
   "source": [
    "test_path = BASE_PREFIX / \"test\"\n",
    "test_fragments = [test_path / fragment_name for fragment_name in test_path.iterdir()]\n",
    "print(\"All fragments:\", test_fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd67e61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.728266Z",
     "iopub.status.busy": "2023-06-14T18:54:33.727968Z",
     "iopub.status.idle": "2023-06-14T18:54:33.760140Z",
     "shell.execute_reply": "2023-06-14T18:54:33.759116Z"
    },
    "papermill": {
     "duration": 0.04134,
     "end_time": "2023-06-14T18:54:33.762480",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.721140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from typing import Union, Optional, Tuple, List\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def get_rect_dset(\n",
    "    fragment_path: Union[Path, str],\n",
    "    z_start: int,\n",
    "    z_dim: int,\n",
    "    buffer: int,\n",
    "    rect: Optional[Tuple[int]] = None,\n",
    "    shuffle: bool = False,\n",
    ") -> data.Dataset:\n",
    "    \"\"\"Get a dataset consisting of a rectangle from a single fragment.\n",
    "    \n",
    "    Args:\n",
    "      fragment_path: Path to folder containing fragment data (e.g., 'data/train/1')\n",
    "      z_start: Lowest z-value to use from the fragment. Should be between 0\n",
    "        and 64. \n",
    "      z_dim: Number of z-values to use.\n",
    "      buffer: Radius of subvolumes in x and y directions. Thus, each item in\n",
    "        the dataset will be a subvolume of size 2*buffer+1 x 2*buffer+1 x z_dim.\n",
    "      rect: Rectangle to use from the fragment. Should be a tuple of the form:\n",
    "          (top_left_corner_x, top_left_corner_y, width, height)\n",
    "        Use show_labels_with_rects() to double-check the rectangle. If rect is\n",
    "        None, the whole dataset will be used.\n",
    "      shuffle: Whether to shuffle the dataset before returning it.\n",
    "    \n",
    "    Returns:\n",
    "      A dataset consisting of subvolumes from the given fragment inside the\n",
    "      given rectangle.\n",
    "    \"\"\"\n",
    "    # clean input\n",
    "    fragment_path = Path(fragment_path)\n",
    "    \n",
    "    images = [\n",
    "        np.array(Image.open(filename), dtype=np.float32)/65535.0\n",
    "        for filename\n",
    "        in tqdm(\n",
    "            sorted((fragment_path / \"surface_volume\").glob(\"*.tif\"))[z_start : z_start + z_dim],\n",
    "            desc=f\"Loading fragment from {fragment_path}\"\n",
    "        )\n",
    "    ]\n",
    "        \n",
    "    # turn images to tensors\n",
    "    image_stack = torch.stack([torch.from_numpy(image) for image in images], \n",
    "                              dim=0)\n",
    "\n",
    "    # get mask and labels\n",
    "    mask = np.array(Image.open(fragment_path / \"mask.png\").convert('1'))\n",
    "    if os.path.exists(fragment_path / \"inklabels.png\"):\n",
    "        label = torch.from_numpy(\n",
    "            np.array(Image.open(fragment_path / \"inklabels.png\"))\n",
    "        ).float()\n",
    "    else:\n",
    "        label = None\n",
    "\n",
    "    # Create a Boolean array of the same shape as the bitmask, initially all True\n",
    "    not_border = np.zeros(mask.shape, dtype=bool)\n",
    "    not_border[buffer:mask.shape[0]-buffer, buffer:mask.shape[1]-buffer] = True\n",
    "    arr_mask = mask * not_border\n",
    "    if rect is not None:\n",
    "        inside_rect = np.zeros(mask.shape, dtype=bool) * arr_mask\n",
    "        # Sets all indexes with inside_rect array to True\n",
    "        inside_rect[rect[1]:rect[1]+rect[3]+1, rect[0]:rect[0]+rect[2]+1] = True\n",
    "        # Set the pixels within the inside_rect to False\n",
    "        pixels = torch.tensor(np.argwhere(inside_rect))\n",
    "    else:\n",
    "        pixels = torch.tensor(np.argwhere(arr_mask))\n",
    "        \n",
    "    if shuffle:\n",
    "        perm = torch.randperm(len(pixels))\n",
    "        pixels = pixels[perm]\n",
    "\n",
    "    # define dataset\n",
    "    return SubvolumeDataset(image_stack, label, pixels, buffer)\n",
    "\n",
    "def predict_on_test_fragments(model: torch.nn.Module,\n",
    "                              fragment_path: Union[Path, str],\n",
    "                              z_start: int,\n",
    "                              z_dim: int,\n",
    "                              buffer: int, \n",
    "                              batch_size: int,\n",
    "                              decision_boundary: float = 0.4) -> np.ndarray:\n",
    "    \"\"\"Generate predictions on a fragment.\n",
    "    \n",
    "    Args:\n",
    "      model: Model to use for prediction.\n",
    "      fragment_path: Path to fragment (e.g. '/data/test/a').\n",
    "      z_start: z-value of lowest fragment layer to use.\n",
    "      z_dim: Number of layers to use.\n",
    "      buffer: Radius of subvolumes in x and y direction. Thus, each subvolume\n",
    "        is an array of shape (z_dim, 2*buffer+1, 2*buffer+1).\n",
    "      batch_size: Number of subvolumes in one batch.\n",
    "      decision_boundary: Decision boundary for predictions. If the model\n",
    "        returns a probability above this number for a given pixel, we classify\n",
    "        that pixel as containing ink.\n",
    "    \n",
    "    Returns:\n",
    "      Predictions on the fragment, as a boolean NumPy array.\n",
    "    \"\"\"\n",
    "    fragment_path = Path(fragment_path)\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    test_dset = get_rect_dset(fragment_path, z_start=z_start,\n",
    "                              z_dim=z_dim, buffer=buffer)\n",
    "    test_loader = data.DataLoader(test_dset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for i, (subvolumes, _) in enumerate(tqdm(test_loader, desc=f\"Predicting on fragment {fragment_path}\")):\n",
    "            output = model(subvolumes.to(DEVICE)).view(-1).sigmoid().cpu().numpy()\n",
    "            outputs.append(output)\n",
    "    image_shape = test_dset.image_stack[0].shape\n",
    "\n",
    "    pred_image = np.zeros(image_shape, dtype=np.uint8)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    for (y, x, _), prob in zip(test_dset.pixels[:outputs.shape[0]], outputs):\n",
    "        pred_image[y, x] = prob > decision_boundary\n",
    "    pred_images.append(pred_image)\n",
    "\n",
    "    print(\"Finished with fragment\", test_fragment)\n",
    "    return pred_image\n",
    "                                            \n",
    "    \n",
    "def rle(output: np.ndarray) -> str:\n",
    "    \"\"\"Turn a NumPy array of booleans to a run-length encoded string.\"\"\"\n",
    "    flat_img = np.where(output > 0.4, 1, 0).astype(np.uint8)\n",
    "    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "    starts_ix = np.where(starts)[0] + 2\n",
    "    ends_ix = np.where(ends)[0] + 2\n",
    "    lengths = ends_ix - starts_ix\n",
    "    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))\n",
    "\n",
    "                                                \n",
    "def make_csv(pred_images: List[np.ndarray], save_path: Union[Path, str, None]):\n",
    "    \"\"\"Save a list of predicted images as a run-length encoded CSV file.\n",
    "    \n",
    "    Args:\n",
    "      pred_images: List of two NumPy arrays of predictions (which we assume\n",
    "        correspond to test fragments a and b respectively).\n",
    "      save_path: Path to save CSV file (e.g. '/kaggle/working/submission.csv').\n",
    "        If None, the file will not be saved.\n",
    "    Returns:\n",
    "      A DataFrame containing run-length encodings of the two fragments.\n",
    "    \"\"\"\n",
    "    submission = defaultdict(list)\n",
    "    for fragment_id, fragment_name in enumerate(['a', 'b']):\n",
    "        submission[\"Id\"].append(fragment_name)\n",
    "        submission[\"Predicted\"].append(rle(pred_images[fragment_id]))\n",
    "    dataframe = pd.DataFrame.from_dict(submission)\n",
    "    dataframe.to_csv(save_path, index=False)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04450086",
   "metadata": {
    "_cell_guid": "98b187d3-1728-47e3-b2ef-333f04d9a907",
    "_uuid": "cf6421bc-8930-4a97-b03d-9ad35a34276d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-06-14T18:54:33.776668Z",
     "iopub.status.busy": "2023-06-14T18:54:33.775632Z",
     "iopub.status.idle": "2023-06-14T18:54:33.812600Z",
     "shell.execute_reply": "2023-06-14T18:54:33.811510Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.046615,
     "end_time": "2023-06-14T18:54:33.815393",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.768778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN_RUN:\n",
    "    pred_images = []\n",
    "    model.eval()\n",
    "    for test_fragment in test_fragments:\n",
    "        pred_images.append(predict_on_test_fragments(model, test_fragment, Z_START, Z_DIM, BUFFER, BATCH_SIZE,\n",
    "                                                     decision_boundary=0.4))\n",
    "    make_csv(pred_images, '/kaggle/working/submission.csv')\n",
    "    plt.imshow(pred_images[0], cmap='gray')\n",
    "else:\n",
    "    import shutil\n",
    "    shutil.copy(src='/kaggle/input/submissioncsv/submission_InkDetector_full_dataset_augmented_pretrained_100000_epochs.csv',\n",
    "                dst='/kaggle/working/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393eefe",
   "metadata": {
    "_cell_guid": "1da918ef-1f34-4d2a-b7f3-9f826b9de1b6",
    "_uuid": "84cb4118-e2e2-4efb-91a9-af0e270e0a7e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0052,
     "end_time": "2023-06-14T18:54:33.826144",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.820944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff4adb62",
   "metadata": {
    "_cell_guid": "ed83ebc7-a0b1-4434-aba0-875bbaea0b52",
    "_uuid": "8bf1d812-b298-4261-8e25-b44f07dd9002",
    "papermill": {
     "duration": 0.005259,
     "end_time": "2023-06-14T18:54:33.837174",
     "exception": false,
     "start_time": "2023-06-14T18:54:33.831915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.26293,
   "end_time": "2023-06-14T18:54:34.964508",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-14T18:54:19.701578",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
