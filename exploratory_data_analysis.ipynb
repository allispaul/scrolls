{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc64391",
   "metadata": {},
   "source": [
    "## Exploratory data analysis of the fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748c8e0",
   "metadata": {},
   "source": [
    "During the model training process, we discovered that certain layers were more crucial for ink detection than others. Specifically, the middle layers were found to be more relevant to the ink detection problem. As a result, we sought to investigate this phenomenon further through exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8507826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX= 'kaggle//input//vesuvius-challenge//train//' \n",
    "BUFFER = 30  # Buffer size in x and y direction\n",
    "Z_START = 16 # First slice in the z direction to use\n",
    "Z_DIM = 32   # Number of slices in the z direction\n",
    "TRAINING_EPOCHS = 20000\n",
    "VALIDATION_EPOCHS= 500\n",
    "LEARNING_RATE = 0.03\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rectangle=[(1100, 3500, 700, 950),(1000, 1000, 1200, 1200),(1500, 2500, 1200, 1200)] # Put the correct coordinates for the rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = gc.collect()\n",
    " \n",
    "# Prints Garbage collector\n",
    "# as 0 object\n",
    "print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b1857",
   "metadata": {},
   "source": [
    "# Mean pixel value plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfeec44",
   "metadata": {},
   "source": [
    "Our initial approach involved examining the relationship between mean pixel values and layers. We generated these plots for each of the three fragments, which can be accessed at [Fragment 1 mean pixel values](mean_pixel_value_frag_1.png), [Fragment 2 mean pixel values](mean_pixel_value_frag_2.png), and [Fragment 3 mean pixel values](mean_pixel_value_frag_3.png).\n",
    "\n",
    "Subsequently, when a new mask became available, we reproduced these plots. The updated versions are now accessible at [Fragment 1 mean pixel values](new_mean_pixel_value_frag_1.png), [Fragment 2 mean pixel values](new_mean_pixel_value_frag_2.png), and [Fragment 3 mean pixel values](new_mean_pixel_value_frag_3.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the fragments\n",
    "for j in range(1, 4):\n",
    "    i = 0\n",
    "    \n",
    "    # Perform garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)\n",
    "    \n",
    "    # Read mask image\n",
    "    mask = np.array(Image.open(PREFIX + str(j) + \"//mask.png\").convert('1'))\n",
    "    multiplier = (mask.shape[0] * mask.shape[1]) / mask.sum()\n",
    "    a = []\n",
    "    \n",
    "    # Loop over a sorted list of files\n",
    "    for filename in sorted(glob.glob(PREFIX + str(j) + \"//surface_volume/*.tif\")):\n",
    "        # Read image file and convert pixel values\n",
    "        df = pd.DataFrame(np.array(Image.open(filename), dtype=np.float32) / 65535.0 * mask)\n",
    "        \n",
    "        # Calculate mean pixel values\n",
    "        df_mean = df.mean() * multiplier\n",
    "        a.append(df_mean.mean())\n",
    "        \n",
    "    # Plot the data\n",
    "    plt.plot(np.array(a))\n",
    "    plt.title(f\"Frag_{str(j)}: The mean pixel values vs layers\")\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Mean pixel value\")\n",
    "    \n",
    "    # Save the plot to a file and display it\n",
    "    plt.savefig(f\"mean_pixel_value_frag_{str(j)}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4193f54",
   "metadata": {},
   "source": [
    "## Correlation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07360b2",
   "metadata": {},
   "source": [
    "Finally, we graphed the correlation of each layer against its corresponding layer number. These visualizations can be found under [Correlation plot frag 1](Correlation_value_frag_1.png), [Correlation plot frag 2](Correlation_value_frag_2.png), and [Correlation plot frag 3](Correlation_value_frag_3.png).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237082c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Loop over the fragments\n",
    "for j in range(1, 4):\n",
    "    i = 0\n",
    "    \n",
    "    # Perform garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(\"Garbage collector: collected\",\n",
    "          \"%d objects.\" % collected)\n",
    "    \n",
    "    # Read mask and labels images\n",
    "    mask = np.array(Image.open(PREFIX + str(j) + \"//mask.png\").convert('1'))\n",
    "    labels = (np.array(Image.open(PREFIX + str(j) + \"//inklabels.png\").convert('1'))).flatten()\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    # Loop over a sorted list of files\n",
    "    for filename in sorted(glob.glob(PREFIX + str(j) + \"//surface_volume/*.tif\")):\n",
    "        # Read image file and convert pixel values\n",
    "        X = (np.array(Image.open(filename), dtype=np.float32) / 65535.0 * mask).flatten()\n",
    "        \n",
    "        # Calculate Pearson correlation coefficient\n",
    "        a.append(stats.pearsonr(X, labels)[0])\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.plot(np.array(a))\n",
    "    plt.title(f\"Frag_{str(j)}: Correlation vs layers\")\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    \n",
    "    # Save the plot to a file and display it\n",
    "    plt.savefig(f\"Correlation_value_frag_{str(j)}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7d612",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05347c",
   "metadata": {},
   "source": [
    "These layers allow us to conclude that the middle layers are indeed responsible for most of the in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
